# -*- coding: utf-8 -*-
"""General_Baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YBieP1EyI9aWz6z5VkH3ro7UjjC1Xcp9
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

!cp /content/drive/MyDrive/lovasz.py /content/
!unzip /content/drive/MyDrive/hubmap-256x256.zip -d /content/
!unzip /content/drive/MyDrive/train.csv.zip -d /content/
!cp /content/drive/MyDrive/sample_submission.csv /content/
!cp /content/drive/MyDrive/HuBMAP-20-dataset_information.csv /content/
# !mkdir /content/drive/MyDrive/result

!pip install git+https://github.com/albumentations-team/albumentations.git
!pip install git+https://github.com/qubvel/segmentation_models.pytorch

from sklearn.model_selection import GroupKFold
import torch
from torch import nn
import torchvision
import cv2
import os
import numpy as np
import pandas as pd
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.sampler import SequentialSampler, RandomSampler
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts
from scipy.ndimage.interpolation import zoom
import albumentations as A
from torch.nn import functional as F
from albumentations.pytorch import ToTensorV2
from lovasz import lovasz_hinge
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler


import segmentation_models_pytorch as smp
from segmentation_models_pytorch import Unet, FPN

import matplotlib.pyplot as plt
import sys
import time
import random

class CFG:
    data = 256 # 512
    debug = False
    num_workers = 2
    img_size = 256 # appropriate input size for encoder 
    scheduler = 'CosineAnnealingWarmRestarts' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']
    epoch = 20 # Change epochs
    criterion = 'Lovasz' #'DiceBCELoss' # ['DiceLoss', 'Hausdorff', 'Lovasz']
    base_model = 'Unet' # ['Unet']
    encoder = 'se_resnext50_32x4d' # ['efficientnet-b5'] or other encoders from smp "se_resnext50_32x4d"
    lr = 1e-4
    min_lr = 1e-6
    batch_size = 32
    weight_decay = 1e-6
    seed = 2021
    n_fold = 5
    train = True
    inference = False
    optimizer = 'Adam'
    T_0 = 20
    T_max = 10
    smoothing = 1
    verbose_step = 1

def seed_torch(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True
    
seed_torch(seed=CFG.seed)

base_transform = A.Compose([
        A.Resize(CFG.img_size, CFG.img_size, p=1.0),
        A.HorizontalFlip(),
        A.VerticalFlip(),
        A.RandomRotate90(),
        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=20, p=0.9, 
                         border_mode=cv2.BORDER_REFLECT),
        A.OneOf([
            A.OpticalDistortion(p=0.4),
            A.GridDistortion(p=.1),
            A.IAAPiecewiseAffine(p=0.4),
        ], p=0.3),
        A.OneOf([
            A.HueSaturationValue(10,15,10),
            A.CLAHE(clip_limit=3),
            A.RandomBrightnessContrast(),            
        ], p=0.5),
        ToTensorV2()
    ], p=1.0)

strong_transform = A.Compose([
            A.Transpose(p=0.5),
            A.VerticalFlip(p=0.5),
            A.HorizontalFlip(p=0.5),
            A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),
            A.OneOf([
                    A.RandomGamma(),
                    A.GaussNoise()           
                ], p=0.5),
            A.OneOf([
                    A.OpticalDistortion(p=0.4),
                    A.GridDistortion(p=0.2),
                    A.IAAPiecewiseAffine(p=0.4),
                ], p=0.5),
            A.OneOf([
                    A.HueSaturationValue(10,15,10),
                    A.CLAHE(clip_limit=4),
                    A.RandomBrightnessContrast(),            
                ], p=0.5),
    
            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),
            A.Resize(CFG.img_size, CFG.img_size, p=1.0),
            ToTensorV2()
        ])

weak_transform = A.Compose([
        A.Resize(CFG.img_size, CFG.img_size, p=1.0),
        A.HorizontalFlip(),
        A.VerticalFlip(),
        A.RandomRotate90(),
        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, 
                         border_mode=cv2.BORDER_REFLECT),
        ToTensorV2()
    ], p=1.0)

val_transform = A.Compose([
        A.Resize(CFG.img_size, CFG.img_size, p=1.0),
        ToTensorV2()
    ], p=1.0)

mean = np.array([0.65459856,0.48386562,0.69428385])
std = np.array([0.15167958,0.23584107,0.13146145])

class HuBMAPDataset(Dataset):
    def __init__(self,df, train='train', augment='weak', transform=True):
        ids = df.id.values
        #kf = KFold(n_splits=nfolds,random_state=SEED,shuffle=True)
        #ids = set(ids[list(kf.split(ids))[fold][0 if train else 1]])
        if CFG.data == 512:
            self.fnames = [fname for fname in os.listdir('/content/train') if fname.split('_')[0] in ids]
        elif CFG.data == 256:
            self.fnames = [fname for fname in os.listdir('/content/train') if fname.split('_')[0] in ids]
        self.train = train
        self.augment = augment
        self.transform = transform
        
    def __len__(self):
        return len(self.fnames)
    
    def __getitem__(self, idx):
        fname = self.fnames[idx]
        if CFG.data == 512:
            img = cv2.cvtColor(cv2.imread(os.path.join('/content/train',fname)), cv2.COLOR_BGR2RGB)
            mask = cv2.imread(os.path.join('/content/masks',fname), cv2.IMREAD_GRAYSCALE)
        elif CFG.data == 256:
            img = cv2.cvtColor(cv2.imread(os.path.join('/content/train',fname)), cv2.COLOR_BGR2RGB)
            mask = cv2.imread(os.path.join('/content/masks',fname), cv2.IMREAD_GRAYSCALE)
        
        if self.train == 'train':
            if self.transform == True:
                if self.augment == 'base':
                    augmented = base_transform(image=img, mask=mask)
                    img, mask = augmented['image'], augmented['mask']
                elif self.augment == 'weak':
                    augmented = weak_transform(image=img, mask=mask)
                    img, mask = augmented['image'], augmented['mask']
                elif self.augment == 'strong':
                    augmented = strong_transform(image=img, mask=mask)
                    img, mask = augmented['image'], augmented['mask']
                    
        elif self.train == 'val':
            transformed = val_transform(image=img, mask=mask)
            img, mask = transformed['image'], transformed['mask']
            
        img = img.type('torch.FloatTensor')
        img = img / 255
        mask = mask.type('torch.FloatTensor')

        return img, mask

train_df = pd.read_csv('/content/train.csv')
# train_df.head()

if CFG.data == 512:
    directory_list = os.listdir('/content/train')
elif CFG.data == 256:
    directory_list = os.listdir('/content/train')
directory_list = [fnames.split('_')[0] for fnames in directory_list]
dir_df = pd.DataFrame(directory_list, columns=['id'])
# dir_df

if CFG.base_model =='Unet':
    base_model = smp.Unet(CFG.encoder, encoder_weights='imagenet', classes=1)
if CFG.base_model =='FPN':
    base_model = smp.FPN(CFG.encoder, encoder_weights='imagenet', classes=1)
# print(base_model)

class HuBMAP(nn.Module):
    def __init__(self):
        super(HuBMAP, self).__init__()
        self.cnn_model = base_model
        
        #self.cnn_model.decoder.blocks.append(self.cnn_model.decoder.blocks[-1])
        #self.cnn_model.decoder.blocks[-2] = self.cnn_model.decoder.blocks[-3]
    
    def forward(self, imgs):
        img_segs = self.cnn_model(imgs)
        return img_segs

class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=CFG.smoothing):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
        inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).sum()                            
        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        
        return dice
    

class DiceBCELoss(nn.Module):
    # Formula Given above.
    def __init__(self, weight=None, size_average=True):
        super(DiceBCELoss, self).__init__()

    def forward(self, inputs, targets, smooth=CFG.smoothing):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
        inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).mean()                            
        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  
        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')
        Dice_BCE = BCE + dice_loss
        
        return Dice_BCE.mean()


class symmetric_lovasz(nn.Module):
    def __init__(self):
        super(symmetric_lovasz, self).__init__()

    def forward(self, outputs, targets):
        return 0.5 * (lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs.float(), 1.0 - targets))

# def symmetric_lovasz(outputs, targets):
#     return 0.5 * (lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1.0 - targets))

if CFG.criterion == 'DiceBCELoss':
    criterion = DiceBCELoss()
elif CFG.criterion == 'DiceLoss':
    criterion = DiceLoss()
# elif CFG.criterion == 'Hausdorff':
#     criterion = Hausdorff_loss()
elif CFG.criterion == 'Lovasz':
    criterion = symmetric_lovasz()

# class Dice_soft(Metric):
#     def __init__(self, axis=1): 
#         self.axis = axis 
#     def reset(self):
#         self.inter, self.union = 0, 0
#     def accumulate(self, learn):
#         pred, targ = flatten_check(torch.sigmoid(learn.pred), learn.y)
#         self.inter += (pred * targ).float().sum().item()
#         self.union += (pred + targ).float().sum().item()
#     @property
#     def value(self):
#         return 2.0 * self.inter / self.union if self.union > 0 else None

def Dice_soft(probability, mask):
    p = probability.reshape(-1)
    t = mask.reshape(-1)

    uion = p.sum() + t.sum()
    
    overlap = (p*t).sum()
    dice = 2 * overlap/(uion+0.001)
    return dice

def Dice_th(probability, mask):

    ths=np.arange(0.1, 0.9, 0.05)
    inter = torch.zeros(len(ths))
    union = torch.zeros(len(ths))

    pred = torch.flatten(torch.sigmoid(probability))
    targ = torch.flatten(mask)


    for i, th in enumerate(ths):
            p = (pred > th).float()
            inter[i] += (p * targ).float().sum().item()
            union[i] += (p + targ).float().sum().item()

    dices = torch.where(union > 0.0, 2.0 * inter / union, torch.zeros_like(union))
    return dices.max(), ths[torch.argmax(dices.max())]

# dice with automatic threshold selection
# class Dice_th(Metric):
#     def __init__(self, ths=np.arange(0.1, 0.9, 0.05), axis=1): 
#         self.axis = axis
#         self.ths = ths
        
#     def reset(self): 
#         self.inter = torch.zeros(len(self.ths))
#         self.union = torch.zeros(len(self.ths))
        
#     def accumulate(self, learn):
#         pred, targ = flatten_check(torch.sigmoid(learn.pred), learn.y)
#         for i, th in enumerate(self.ths):
#             p = (pred > th).float()
#             self.inter[i] += (p * targ).float().sum().item()
#             self.union[i] += (p + targ).float().sum().item()

#     @property
#     def value(self):
#         dices = torch.where(self.union > 0.0, 
#                 2.0 * self.inter / self.union, torch.zeros_like(self.union))
#         return dices.max()

def HuBMAPLoss(images, targets, model, device, loss_func=criterion):
    model.to(device)
    images = images.to(device)
    targets = targets.to(device)
    outputs = model(images)
    loss_func = loss_func
    loss = loss_func(outputs, targets)
    return loss, outputs

def train_one_epoch(epoch, model, device, optimizer, scheduler, trainloader):
    model.train()
    
    t = time.time()
    running_loss = None

    pbar = tqdm(enumerate(trainloader), total=len(trainloader))
    for step, (images, targets) in pbar:

        with autocast():
            loss, outputs = HuBMAPLoss(images, targets, model, device)
            scaler.scale(loss).backward()

            if running_loss is None:
                running_loss = loss.item()
            else:
                running_loss = running_loss * .99 + loss.item() * .01
            
            # if ((step + 1) % 4 == 0 or (step + 1) == len(trainloader)):
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
                
            # loss = loss.detach().item()
            # total_loss += loss
            
            if ((step + 1) % CFG.verbose_step == 0) or ((step + 1) == len(trainloader)):
                    description = f'epoch {epoch} loss: {running_loss:.4f}'

                    pbar.set_description(description)

    if scheduler is not None:
        scheduler.step()
            
        
def valid_one_epoch(epoch, model, device, optimizer, scheduler, validloader):
    model.eval()
    
    t = time.time()
    loss_sum = 0
    sample_num = 0
    val_probability, val_mask = [], []
    
    pbar = tqdm(enumerate(validloader), total=len(validloader))
    for step, (images, targets) in pbar:
        loss, outputs = HuBMAPLoss(images, targets, model, device)
        loss = loss.detach().item()
        
        loss_sum += loss * targets.shape[0]
        sample_num += targets.shape[0]

        output_ny = outputs.data.cpu() # .numpy()
        target_np = targets.data.cpu() # .numpy()
            
        val_probability.append(output_ny)
        val_mask.append(target_np)

        if ((step + 1) % CFG.verbose_step == 0) or ((step + 1) == len(val_loader)):
            description = f'epoch {epoch} loss: {loss_sum / sample_num:.4f}'
            pbar.set_description(description)
    # val_probability = torch.from_numpy(val_probability)
    # val_mask = torch.from_numpy(val_mask)
    val_probability = torch.cat(val_probability)
    val_mask = torch.cat(val_mask)

    best_dice, best_th = Dice_th(val_probability, val_mask)

    print("Dice_soft: ", Dice_soft(val_probability, val_mask), end='    ')
    print("Best_dice: ", best_dice, end='    ')
    print("best_th: ", best_th)

    return best_dice, best_th

gkf = GroupKFold(CFG.n_fold)
dir_df['Folds'] = 0
for fold, (tr_idx, val_idx) in enumerate(gkf.split(dir_df, groups=dir_df[dir_df.columns[0]].values)):
    dir_df.loc[val_idx, 'Folds'] = fold
# dir_df

def prepare_train_valid_dataloader(df, fold):
    train_ids = df[~df.Folds.isin(fold)]
    val_ids = df[df.Folds.isin(fold)]
    
    train_ds = HuBMAPDataset(train_ids, train='train', augment='base', transform=True)
    val_ds = HuBMAPDataset(val_ids, train='val', augment='base', transform=True)
    train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, pin_memory=True, shuffle=True, num_workers=CFG.num_workers)
    val_loader = DataLoader(val_ds, batch_size=CFG.batch_size, pin_memory=True, shuffle=False, num_workers=CFG.num_workers)
    return train_loader, val_loader

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = HuBMAP().to(device)
#optimizer
optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)

# scheduler setting
if CFG.scheduler == 'CosineAnnealingWarmRestarts':
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)
elif CFG.scheduler == 'ReduceLROnPlateau':
    scheduler = ReduceLROnPlateauReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)
elif CFG.scheduler == 'CosineAnnealingLR':
    scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)

for fold, (tr_idx, val_idx) in enumerate(gkf.split(dir_df, groups=dir_df[dir_df.columns[0]].values)):

    trainloader, validloader = prepare_train_valid_dataloader(dir_df, [fold])
    scaler = GradScaler()

    for epoch in range(CFG.epoch):
        best = 0
        train_one_epoch(epoch, model, device, optimizer, scheduler, trainloader)
        with torch.no_grad():
            best_dice, best_th = valid_one_epoch(epoch, model, device, optimizer, scheduler, validloader)
            if best_dice > best:
                best = best_dice
                torch.save(model.state_dict(), f'FOLD-{fold}-model.pth')

